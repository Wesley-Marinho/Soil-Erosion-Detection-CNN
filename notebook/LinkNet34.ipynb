{"cells":[{"cell_type":"code","execution_count":null,"id":"a908578e","metadata":{},"outputs":[],"source":["! pip install numpy torch torchvision torch.utils.data PIL scipy tqdm matplotlib scikit-image"]},{"cell_type":"code","execution_count":1,"id":"7ac14a50","metadata":{"id":"7ac14a50"},"outputs":[],"source":["# Use Google Colab\n","use_google_colab = False\n","# Process the training dataset\n","training_data_processing = True\n","# Train the model\n","model_training = True\n","# Validation the model\n","model_validation = False\n","# Load the model from your Google Drive or local file system\n","model_loading = False"]},{"cell_type":"code","execution_count":2,"id":"05dfeb22","metadata":{"id":"05dfeb22"},"outputs":[],"source":["if use_google_colab:\n","    from google.colab import drive\n","    from google.colab import files\n","    # Mount your Google Drive on your runtime\n","    drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":3,"id":"e03cc21e","metadata":{"id":"e03cc21e"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import torch\n","from torch import nn\n","from torchvision import models\n","from torch.utils.data import DataLoader, TensorDataset\n","from PIL import Image\n","from skimage.transform import resize\n","import scipy\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":9,"id":"z5JSCfmd7YW_","metadata":{"id":"z5JSCfmd7YW_"},"outputs":[],"source":["if use_google_colab:\n","    path_training = '/content/gdrive/MyDrive/proj2_data/training/'\n","    path_testing = '/content/gdrive/MyDrive/proj2_data/test_set_images/'\n","    path_data = '/content/gdrive/MyDrive/proj2_data/data/'\n","    path_model = '/content/gdrive/MyDrive/proj2_data/models/'\n","else:\n","    path_training = '../training/'\n","    path_testing = '../test/'\n","    path_data = '../data/'\n","    path_model = '../models/'"]},{"cell_type":"markdown","id":"b12b480d","metadata":{"id":"b12b480d"},"source":["# Get Device for Training"]},{"cell_type":"code","execution_count":5,"id":"db1c96b5","metadata":{"id":"db1c96b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available. Utilize GPUs for computation\n"]}],"source":["# Determine if your system supports CUDA\n","cuda_available = torch.cuda.is_available()\n","if cuda_available:\n","    print('CUDA is available. Utilize GPUs for computation')\n","    device = torch.device(\"cuda\")\n","else:\n","    print('CUDA is not available. Utilize CPUs for computation.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":6,"id":"37344ed4","metadata":{"id":"37344ed4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Feb  6 13:28:03 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 546.12                 Driver Version: 546.12       CUDA Version: 12.3     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA GeForce RTX 4080      WDDM  | 00000000:01:00.0  On |                  N/A |\n","|  0%   39C    P8              12W / 340W |    422MiB / 16376MiB |      2%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|    0   N/A  N/A      8836    C+G   C:\\Windows\\explorer.exe                   N/A      |\n","|    0   N/A  N/A      9608    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n","|    0   N/A  N/A      9716    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n","|    0   N/A  N/A      9760    C+G   ...__8wekyb3d8bbwe\\Microsoft.Notes.exe    N/A      |\n","|    0   N/A  N/A     11900    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n","|    0   N/A  N/A     12980    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n","|    0   N/A  N/A     13268    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n","|    0   N/A  N/A     14208    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n","|    0   N/A  N/A     14612    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n","|    0   N/A  N/A     16084    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe    N/A      |\n","|    0   N/A  N/A     19180    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["# Get the GPU information\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","else:\n","    print(gpu_info)"]},{"cell_type":"markdown","id":"dcd0e4b0","metadata":{"id":"dcd0e4b0"},"source":["# Define the Neural Network"]},{"cell_type":"code","execution_count":33,"id":"00bec1a0","metadata":{"id":"00bec1a0"},"outputs":[],"source":["\"\"\"\n","LinkNet34.py - Define the neural network for LinkNet34.\n","Reference - https://ieeexplore.ieee.org/abstract/document/8305148\n","\"\"\"\n","\n","class decoder_block(nn.Module):\n","    # Instantiate all the modules\n","    def __init__(self, in_channels, out_channels):\n","        super(decoder_block,self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels // 4, kernel_size=1),\n","            nn.BatchNorm2d(in_channels // 4),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=3, \n","                               stride=2, padding=1, output_padding=1),\n","            nn.BatchNorm2d(in_channels // 4),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.Conv2d(in_channels // 4, out_channels, kernel_size=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","    \n","    # Define the block structure\n","    def forward(self, x):\n","        \"\"\"\n","        decoder_block's forward function.\n","        Args:\n","            x (tensor): input tensor\n","        Returns:\n","            x (tensor): the output of this block after processing\n","        \"\"\"\n","        x = self.block(x)\n","        return x\n","\n","\n","class LinkNet34(nn.Module):\n","    # Instantiate all the modules\n","    def __init__(self):\n","        super(LinkNet34, self).__init__()\n","        # Construct a ResNet-34 architecture from https://arxiv.org/pdf/1512.03385.pdf\n","        # Return a model pre-trained on ImageNet\n","        resnet = models.resnet34(pretrained=True)\n","\n","        # Input Block\n","        self.input_block = nn.Sequential(*list(resnet.children())[0:4])\n","\n","        # Encoder Blocks\n","        self.encoder1 = nn.Sequential(*list(resnet.children())[4])\n","        self.encoder2 = nn.Sequential(*list(resnet.children())[5])\n","        self.encoder3 = nn.Sequential(*list(resnet.children())[6])\n","        self.encoder4 = nn.Sequential(*list(resnet.children())[7])\n","\n","        # Decoder Blocks\n","        \n","        self.decoder4 = decoder_block(512, 256)\n","        self.decoder3 = decoder_block(256, 128)\n","        self.decoder2 = decoder_block(128, 64)\n","        self.decoder1 = decoder_block(64, 64)\n","\n","        # Output Block\n","        self.output_block = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 32, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 1, kernel_size=2, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    # Define the network structure\n","    def forward(self, x):\n","        \"\"\"\n","        LinkNet34's forward function.\n","        Args:\n","            x (tensor): input tensor\n","        Returns:\n","            o1 (tensor): the output of this model after processing\n","        \"\"\"\n","        # Input\n","        i1 = self.input_block(x)\n","\n","\n","        # Encoding\n","        e1 = self.encoder1(i1)\n","        e2 = self.encoder2(e1)\n","        e3 = self.encoder3(e2)\n","        e4 = self.encoder4(e3)\n","\n","        # Decoding\n","        d4 = self.decoder4(e4) + e3\n","        d3 = self.decoder3(d4) + e2\n","        d2 = self.decoder2(d3) + e1\n","        d1 = self.decoder1(d2)\n","\n","        # Output\n","        o1 = self.output_block(d1)\n","\n","        return o1"]},{"cell_type":"markdown","id":"68b4d932","metadata":{"id":"68b4d932"},"source":["# Create an Instance of the Neural Network"]},{"cell_type":"code","execution_count":32,"id":"ac408fb2","metadata":{"id":"ac408fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["LinkNet34(\n","  (input_block): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  )\n","  (encoder1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (encoder2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (encoder3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (encoder4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (decoder5): decoder_block(\n","    (block): Sequential(\n","      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (8): ReLU(inplace=True)\n","    )\n","  )\n","  (decoder4): decoder_block(\n","    (block): Sequential(\n","      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n","      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (8): ReLU(inplace=True)\n","    )\n","  )\n","  (decoder3): decoder_block(\n","    (block): Sequential(\n","      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (8): ReLU(inplace=True)\n","    )\n","  )\n","  (decoder2): decoder_block(\n","    (block): Sequential(\n","      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (8): ReLU(inplace=True)\n","    )\n","  )\n","  (decoder1): decoder_block(\n","    (block): Sequential(\n","      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n","      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n","      (7): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (8): ReLU(inplace=True)\n","    )\n","  )\n","  (output_block): Sequential(\n","    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n","    (5): Sigmoid()\n","  )\n",")\n"]}],"source":["model = LinkNet34()\n","if cuda_available:\n","    # Move the model to GPU\n","    model.cuda()\n","print(model)"]},{"cell_type":"markdown","id":"51deced5","metadata":{"id":"51deced5"},"source":["# Load and Process the Training Dataset"]},{"cell_type":"code","execution_count":37,"id":"75a606eb","metadata":{"id":"75a606eb"},"outputs":[],"source":["# The resolution of resized training images and the corresponding masks\n","training_resize = 512\n","# The number of resized training pairs used for data augmentation\n","training_number = 367\n","# The resolution of resized testing images\n","testing_resize = int(608 * training_resize / 400)\n","if testing_resize % 2 == 1:\n","    testing_resize += 1"]},{"cell_type":"code","execution_count":38,"id":"7ee7b917","metadata":{"id":"7ee7b917"},"outputs":[],"source":["def training_data_loading(path_training, training_resize, training_number):\n","    \"\"\"\n","    training_data_loading - Load and generate the resized training dataset and validation dataset.\n","    Args:\n","        path_training (str): the location in your Google Drive or local file system\n","        training_resize (int): the resolution of resized training images and their corresponding masks (training pairs) (default: 384)\n","        training_number (int): the number of resized training pairs used for data augmentation (default: 100)\n","    Returns:\n","        images_training, labels_training (numpy): the resized training dataset\n","        images_validation, labels_validation (numpy): the resized validation dataset\n","    \"\"\"\n","    images_loading = np.empty(shape=(525, 4, training_resize, training_resize))\n","    labels_loading = np.empty(shape=(525, 1, training_resize, training_resize))\n","    \n","    for index in tqdm(range(1, 525)):\n","        # Load a training pair\n","        image = np.array(Image.open(f'{path_training}images/({str(index)}).png')).astype(float) / 255\n","        label = np.array(Image.open(f'{path_training}groundtruth/({str(index)}).png')).astype(float) / 255\n","        \n","        # Expand the shape of the mask\n","        label = np.expand_dims(label, 2)\n","        \n","        # Resize the training pair\n","        image = resize(image, (training_resize, training_resize))\n","        label = resize(label, (training_resize, training_resize))\n","        \n","        # Reverse the axes of the resized training pair\n","        image = np.transpose(image, (2, 0, 1))\n","        label = np.transpose(label, (2, 0, 1))\n","        \n","        images_loading[index-1] = image\n","        labels_loading[index-1] = label\n","    \n","    # Permute the resized training dataset randomly\n","    permuted_sequence = np.random.permutation(training_number)\n","    images_loading = images_loading[permuted_sequence]\n","    labels_loading = labels_loading[permuted_sequence]\n","    \n","    # Generate the resized training dataset and validation dataset\n","    images_training = images_loading[:training_number]\n","    labels_training = labels_loading[:training_number]\n","    images_validation = images_loading[training_number:]\n","    labels_validation = labels_loading[training_number:]\n","    \n","    return images_training, labels_training, images_validation, labels_validation\n","\n","\n","def training_data_augmentation(images_training, labels_training, rotations, flips, shifts, training_resize):\n","    \"\"\"\n","    training_data_augmentation - Generate the augmented training dataset.\n","    Args:\n","        images_training, labels_training (numpy): the resized training dataset\n","        rotations (list): the parameters for rotating resized training images and their corresponding masks (training pairs)\n","        flips (list): the parameters for flipping rotated training pairs\n","        shifts (list): the parameters for shifting flipped training pairs\n","        training_resize (int): the resolution of resized training pairs (default: 384)\n","    Returns:\n","        images_augmented, labels_augmented (numpy): the augmented training dataset\n","    \"\"\"\n","    num_rota = len(rotations)\n","    num_flip = len(flips)\n","    num_shft = len(shifts)\n","    \n","    # Generate the augmented training dataset\n","    num_training = images_training.shape[0]\n","    num_augmented = num_training * num_rota * num_flip * num_shft\n","    images_augmented = np.empty(shape=(num_augmented, 4, training_resize, training_resize))\n","    labels_augmented = np.empty(shape=(num_augmented, 1, training_resize, training_resize))\n","    print(f\"images_augmented.shape = {images_augmented.shape}\")\n","    print(f\"labels_augmented.shape = {labels_augmented.shape}\")\n","    \n","    counter = 0\n","    for index in tqdm(range(num_training)):\n","        image = np.transpose(images_training[index], (1, 2, 0))\n","        label = np.transpose(labels_training[index], (1, 2, 0))\n","        for rota in rotations:\n","            for flip in flips:\n","                for shft in shifts:\n","                    # Rotate a resized training pair\n","                    image_rota = scipy.ndimage.rotate(image, rota, reshape=False, mode='reflect')\n","                    label_rota = scipy.ndimage.rotate(label, rota, reshape=False, mode='reflect')\n","                    \n","                    # Flip the rotated training pair\n","                    if flip == 'original':\n","                        image_flip = image_rota\n","                        label_flip = label_rota\n","                    else:\n","                        image_flip = flip(image_rota)\n","                        label_flip = flip(label_rota)\n","                    \n","                    # Shift the flipped training pair\n","                    shft_H = np.random.uniform(low=shft[0], high=shft[1], size=1)[0]\n","                    shft_W = np.random.uniform(low=shft[0], high=shft[1], size=1)[0]\n","                    image_shft = scipy.ndimage.shift(image_flip, (shft_H, shft_W, 0), mode='reflect')\n","                    label_shft = scipy.ndimage.shift(label_flip, (shft_H, shft_W, 0), mode='reflect')\n","                    \n","                    images_augmented[counter] = np.clip(np.transpose(image_shft, (2, 0, 1)), 0, 1)\n","                    labels_augmented[counter] = np.transpose(label_shft, (2, 0, 1)) > 0.3\n","                    counter += 1\n","    \n","    # Permute the augmented training dataset randomly\n","    permuted_sequence = np.random.permutation(num_augmented)\n","    images_augmented = images_augmented[permuted_sequence]\n","    labels_augmented = labels_augmented[permuted_sequence]\n","    \n","    return images_augmented, labels_augmented"]},{"cell_type":"code","execution_count":36,"id":"91f46696","metadata":{"id":"91f46696"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 524/524 [00:34<00:00, 15.11it/s]\n"]},{"ename":"MemoryError","evalue":"Unable to allocate 8.60 GiB for an array with shape (4404, 1, 512, 512) and data type float64","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[36], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m flips \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mflipud, np\u001b[38;5;241m.\u001b[39mfliplr] \u001b[38;5;66;03m# 'original', np.flipud, np.fliplr\u001b[39;00m\n\u001b[0;32m     11\u001b[0m shifts \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m)]\n\u001b[1;32m---> 13\u001b[0m images_augmented, labels_augmented \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_data_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mlabels_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mrotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mflips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mshifts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mtraining_resize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Save the augmented training dataset and resized validation dataset\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# to your Google Drive or local file system\u001b[39;00m\n\u001b[0;32m     21\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mimages_training\u001b[39m\u001b[38;5;124m'\u001b[39m, images_augmented)\n","Cell \u001b[1;32mIn[35], line 68\u001b[0m, in \u001b[0;36mtraining_data_augmentation\u001b[1;34m(images_training, labels_training, rotations, flips, shifts, training_resize)\u001b[0m\n\u001b[0;32m     66\u001b[0m num_augmented \u001b[38;5;241m=\u001b[39m num_training \u001b[38;5;241m*\u001b[39m num_rota \u001b[38;5;241m*\u001b[39m num_flip \u001b[38;5;241m*\u001b[39m num_shft\n\u001b[0;32m     67\u001b[0m images_augmented \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(shape\u001b[38;5;241m=\u001b[39m(num_augmented, \u001b[38;5;241m4\u001b[39m, training_resize, training_resize))\n\u001b[1;32m---> 68\u001b[0m labels_augmented \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_augmented\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_resize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_resize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages_augmented.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages_augmented\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_augmented.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels_augmented\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.60 GiB for an array with shape (4404, 1, 512, 512) and data type float64"]}],"source":["if training_data_processing:\n","    # Load and generate the resized training dataset and validation dataset\n","    images_training, labels_training, images_validation, labels_validation = training_data_loading(path_training,\n","                                                                                                   training_resize,\n","                                                                                                   training_number)\n","    # Generate the augmented training dataset\n","    rotations = [0, 45, 90, 135] # the rotation angle\n","    \n","    flips = ['original', np.flipud, np.fliplr] # 'original', np.flipud, np.fliplr\n","\n","    shifts = [(-16, 16)]\n","    \n","    images_augmented, labels_augmented = training_data_augmentation(images_training, \n","                                                                    labels_training, \n","                                                                    rotations, \n","                                                                    flips, \n","                                                                    shifts, \n","                                                                    training_resize)\n","    # Save the augmented training dataset and resized validation dataset\n","    # to your Google Drive or local file system\n","    np.save(f'{path_data}images_training', images_augmented)\n","    np.save(f'{path_data}labels_training', labels_augmented)\n","    np.save(f'{path_data}images_validation', images_validation)\n","    np.save(f'{path_data}labels_validation', labels_validation)\n","elif not model_loading:\n","    # Load the augmented training dataset and resized validation dataset\n","    # from your Google Drive or local file system\n","    images_augmented = np.load(f'{path_data}images_training.npy')\n","    labels_augmented = np.load(f'{path_data}labels_training.npy')\n","    images_validation = np.load(f'{path_data}images_validation.npy')\n","    labels_validation = np.load(f'{path_data}labels_validation.npy')"]},{"cell_type":"markdown","id":"0ac8f763","metadata":{"id":"0ac8f763"},"source":["# Train the Instance of the Neural Network"]},{"cell_type":"code","execution_count":26,"id":"67068793","metadata":{"id":"67068793"},"outputs":[],"source":["class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","        self.bce_loss = nn.BCELoss()\n","\n","    def forward(self, outputs, targets, smooth=0):\n","        \"\"\"\n","        DiceBCELoss - Compute the Dice-BCE Loss.\n","        Args:\n","            outputs (tensor): output tensor\n","            targets (tensor): target tensor\n","        Returns:\n","            dice_BCE_loss (tensor): the Dice-BCE Loss\n","        \"\"\"\n","        # Flatten output and target tensors\n","        outputs = outputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        # Compute the dice Loss\n","        intersection = (outputs * targets).sum()                           \n","        dice_loss = 1 - (2. * intersection + smooth) / (outputs.sum() + targets.sum() + smooth)\n","        \n","        # Compute the standard binary cross-entropy (BCE) loss\n","        BCE_loss = self.bce_loss(outputs, targets)\n","        \n","        dice_BCE_loss = dice_loss + BCE_loss\n","        \n","        return dice_BCE_loss\n","\n","\n","class BCEIoULoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(BCEIoULoss, self).__init__()\n","        self.bce_loss = nn.BCELoss()\n","\n","    def forward(self, outputs, targets, beta=0.6, alpha=0.25, gamma=4, smooth=0):\n","        \"\"\"\n","        BCEIoULoss - Compute the BCEIoULoss Loss.\n","        Args:\n","            outputs (tensor): output tensor\n","            targets (tensor): target tensor\n","        Returns:\n","            BCE_IoU_loss (tensor): the BCE-IoU Loss\n","        \"\"\"\n","        # Flatten output and target tensors\n","        outputs = outputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        # Compute the intersection-over-union (IoU) loss\n","        intersection = (outputs * targets).sum()\n","        total = (outputs + targets).sum()\n","        union = total - intersection \n","        IoU_loss = 1 - (intersection + smooth) / (union + smooth)\n","        \n","        # Compute the modified BCE loss\n","        BCE_loss = self.bce_loss(outputs, targets)\n","        BCE_exp = torch.exp(-BCE_loss)\n","        modified_BCE_loss = alpha * (1 - BCE_exp) ** gamma * BCE_loss\n","        \n","        BCE_IoU_loss = beta * modified_BCE_loss + (1 - beta) * IoU_loss\n","\n","        return BCE_IoU_loss"]},{"cell_type":"code","execution_count":27,"id":"882daf1f","metadata":{"id":"882daf1f"},"outputs":[],"source":["def train(model,\n","          images_training,\n","          labels_training,\n","          images_validation,\n","          labels_validation,\n","          loss_func,\n","          batch_size=8,\n","          learning_rate=1e-3,\n","          epochs=80,\n","          model_validation=False,\n","          cuda_available=True,\n","          path_model = 'models/'):\n","    \"\"\"\n","    train - Train the instance of the neural network.\n","    Args:\n","        model (torch): the instance of the neural network\n","        images_training, labels_training (numpy): the augmented training dataset\n","        images_validation, labels_validation (numpy): the resized validation dataset\n","        loss_func (class): the loss function\n","        batch_size (int): the number of samples per batch to load (default: 8)\n","        learning_rate (float): the learning rate (default: 1e-3)\n","        epochs (int): the learning epochs (default: 80)\n","        if_validation (bool): the flag indicating whether or not to implement validation (default: False)\n","        cuda_available (bool): the flag indicating whether CUDA is available (default: True)\n","    \"\"\"\n","    # Use torch.utils.data to create a training_generator\n","    images_training = torch.Tensor(images_training)\n","    labels_training = torch.Tensor(labels_training)\n","    training_set = TensorDataset(images_training, labels_training)\n","    training_generator = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n","\n","    # Use torch.utils.data to create a validation_generator\n","    if model_validation and len(images_validation) > 0:\n","        images_validation = torch.Tensor(images_validation)\n","        labels_validation = torch.Tensor(labels_validation)\n","        validation_set = TensorDataset(images_validation, labels_validation)\n","        validation_generator = DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n","\n","    # Implement Adam algorithm\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    # Decay the learning rate by gamma every step_size epochs.\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5, verbose=True)\n","\n","    # Loop over epochs\n","    for epoch in tqdm(range(epochs)):\n","        # Training\n","        print(f'\\n---------Training for Epoch {epoch + 1} starting:---------')\n","        model.train()\n","        loss_training = 0\n","        # Loop over batches in an epoch using training_generator\n","        for index, (inputs, labels) in enumerate(training_generator):\n","            if cuda_available:\n","                # Transfer to GPU\n","                inputs, labels = inputs.cuda(), labels.cuda()\n","            \n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = loss_func(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            loss_training += loss\n","\n","            if index % 20 == 0:\n","                loss_item = loss.item()\n","                print(f'→ Running_loss for Batch {index + 1}: {loss_item}')\n","        \n","        print(f'\\033[1mTraining loss for Epoch {epoch + 1}: {loss_training}\\033[0m\\n')\n","\n","        if model_validation and len(images_validation) > 0:\n","            # Validation\n","            print(f'--------Validation for Epoch {epoch + 1} starting:--------')\n","            model.eval()\n","            with torch.no_grad():\n","                loss_validation = 0\n","                # Loop over batches in an epoch using validation_generator\n","                for index, (inputs, labels) in enumerate(validation_generator):\n","                    if cuda_available:\n","                        # Transfer to GPU\n","                        inputs, labels = inputs.cuda(), labels.cuda()\n","                \n","                    outputs = model(inputs)\n","                    loss_validation += loss_func(outputs, labels)\n","                \n","            print(f'\\033[1mValidation loss for Epoch {epoch + 1}: {loss_validation}\\033[0m\\n')\n","\n","        scheduler.step()\n","                    \n","        torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': loss_func,\n","                }, path_model + 'model.model')"]},{"cell_type":"code","execution_count":30,"id":"33caad3b","metadata":{"id":"33caad3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","images_training.shape = (4404, 4, 384, 384)\n","labels_training.shape = (4404, 1, 384, 384)\n","images_validation.shape = (0, 4, 384, 384)\n","labels_validation.shape = (0, 1, 384, 384)\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","---------Training for Epoch 1 starting:---------\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10 [00:00<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 3, 7, 7], expected input[100, 4, 384, 384] to have 3 channels, but got 4 channels instead","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages_validation.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages_validation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_validation.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels_validation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mimages_augmented\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlabels_augmented\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[43mimages_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlabels_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBCEIoULoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# BCEIoULoss(), DiceBCELoss(), nn.BCELoss()\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcuda_available\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_available\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_model\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[27], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, images_training, labels_training, images_validation, labels_validation, loss_func, batch_size, learning_rate, epochs, model_validation, cuda_available, path_model)\u001b[0m\n\u001b[0;32m     54\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, labels)\n\u001b[0;32m     59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[22], line 81\u001b[0m, in \u001b[0;36mLinkNet34.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mLinkNet34's forward function.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    o1 (tensor): the output of this model after processing\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Input\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m i1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m     84\u001b[0m e1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder1(i1)\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\wesleymarinho\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[100, 4, 384, 384] to have 3 channels, but got 4 channels instead"]}],"source":["if model_training:\n","    print(f\"\\nimages_training.shape = {images_augmented.shape}\")\n","    print(f\"labels_training.shape = {labels_augmented.shape}\")\n","    print(f\"images_validation.shape = {images_validation.shape}\")\n","    print(f\"labels_validation.shape = {labels_validation.shape}\")\n","\n","    train(model,\n","          images_augmented,\n","          labels_augmented,\n","          images_validation,\n","          labels_validation,\n","          loss_func=BCEIoULoss(), # BCEIoULoss(), DiceBCELoss(), nn.BCELoss()\n","          batch_size=100,\n","          learning_rate=1e-3,\n","          epochs=10,\n","          model_validation=model_validation,\n","          cuda_available=cuda_available,\n","          path_model=path_model)"]},{"cell_type":"code","execution_count":null,"id":"4r7atglYKv4-","metadata":{"id":"4r7atglYKv4-"},"outputs":[],"source":["if model_loading:\n","    # Load the model from your Google Drive or local file system\n","    checkpoint = torch.load(path_model + 'model.model')\n","    model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"markdown","id":"A_Evk4YfAyK7","metadata":{"id":"A_Evk4YfAyK7"},"source":["# Process the Testing Dataset and Create the Submission File"]},{"cell_type":"code","execution_count":null,"id":"f6e6fd77","metadata":{"id":"f6e6fd77"},"outputs":[],"source":["def testing_patch_extracting(input, trar=384, tesr=584):\n","    \"\"\"\n","    testing_patch_extracting - Divide each resized testing image into four patches, one at each corner.\n","    Args:\n","        input (numpy) - the resized testing image\n","        trar (int) - the resolution of resized training images and the corresponding masks\n","        tesr (int) - the resolution of resized testing images\n","    Returns:\n","        input_patches (numpy) - the four patches\n","    \"\"\"\n","    if tesr / 2 > trar:\n","        raise AssertionError(\"training_resize is too small.\")\n","\n","    input_patches = np.empty(shape=(4, input.shape[2], trar, trar))\n","    input_patches[0] = np.transpose(input[0:0+trar, 0:0+trar, :], (2, 0, 1))\n","    input_patches[1] = np.transpose(input[0:0+trar, tesr-trar:tesr, :], (2, 0, 1))\n","    input_patches[2] = np.transpose(input[tesr-trar:tesr, 0:0+trar, :], (2, 0, 1))\n","    input_patches[3] = np.transpose(input[tesr-trar:tesr, tesr-trar:tesr, :], (2, 0, 1))\n","    \n","    return input_patches\n","\n","\n","def testing_patch_assembling(output_patches, trar=384, tesr=584):\n","    \"\"\"\n","    testing_patch_assembling - Merge the four masks into one resized mask.\n","    Args:\n","        output_patches (numpy) - the masks of the four patches\n","        trar (int) - the resolution of resized training images and the corresponding masks\n","        tesr (int) - the resolution of resized testing images\n","    Returns:\n","        output (numpy) - the resized mask\n","    \"\"\"\n","    # The extracting length\n","    eL = int(tesr / 2)\n","\n","    output = np.empty(shape=(output_patches.shape[1], tesr, tesr))\n","    output[:, 0:eL, 0:eL] = output_patches[0, :, 0:eL, 0:eL]\n","    output[:, 0:eL, tesr-eL:tesr] = output_patches[1, :, 0:eL, trar-eL:trar]\n","    output[:, tesr-eL:tesr, 0:eL] = output_patches[2, :, trar-eL:trar, 0:eL]\n","    output[:, tesr-eL:tesr, tesr-eL:tesr] = output_patches[3, :, trar-eL:trar, trar-eL:trar]\n","    \n","    return output\n","\n","def mask_to_submission(output, index):\n","    \"\"\"\n","    mask_to_submission - Convert the mask of each testing image into the submission format.\n","    Args:\n","        output (numpy) - the mask of the testing image\n","        index (int) - the index of the testing image\n","    Returns:\n","        mask_submission (list) - the submission format of the mask\n","    \"\"\"\n","    mask_submission = []\n","    for i in range(0, output.shape[0], 16):\n","        for j in range(0, output.shape[1], 16):\n","            prediction = 0\n","            patch = output[j:j+16, i:i+16]\n","            if np.mean(patch > 0.2) > 0.25:\n","                prediction = 1\n","            mask_submission.append([\"{:03d}_{}_{}\".format(index, i, j), prediction])\n","    return mask_submission\n","\n","\n","def submission_creating(model, path_testing='test_set_images/', training_resize=384, testing_resize=584, cuda_available=True):\n","    \"\"\"\n","    submission_creating - Load and generate the resized training dataset and validation dataset.\n","    Args:\n","        model (torch): the instance of the neural network\n","        path_testing (str): the location in your Google Drive or local file system\n","        training_resize (int): the resolution of resized training images and their corresponding masks (training pairs) (default: 384)\n","        testing_resize (int): the resolution of resized testing images (default: 584)\n","        cuda_available (bool): the flag indicating whether CUDA is available (default: True)\n","    Returns:\n","        submission (numpy): the final submission file\n","    \"\"\"\n","    submit_outputs = []\n","    for index in tqdm(range(1, 51)):\n","        model.eval()\n","        # Load a testing image\n","        input = np.array(Image.open(f'{path_testing}/test_{index}/test_{index}.png')).astype('float32') / 255\n","\n","        # Resize the testing image\n","        input = resize(input, (testing_resize, testing_resize))\n","\n","        # Divide the resized testing image into four patches, one at each corner.\n","        input_patches = testing_patch_extracting(input, training_resize, testing_resize)\n","        input_patches = torch.from_numpy(input_patches).float()\n","\n","        # Predict the mask of the four patches\n","        if cuda_available:\n","            output_patches = model(input_patches.cuda()).detach().cpu().numpy()\n","        else:\n","            output_patches = model(input_patches).detach().numpy()\n","        \n","        # Merge the four masks into one resized mask\n","        output = testing_patch_assembling(output_patches, training_resize, testing_resize)[0, :, :]\n","\n","        # Restore the resized mask to the original resolution\n","        output = resize(output, (608, 608))\n","\n","        # Convert the mask of the testing image into the submission format\n","        submit_output = mask_to_submission(output, index)\n","\n","        submit_outputs.append(submit_output)\n","\n","    submission = np.concatenate(submit_outputs, axis=0)\n","    submission = np.concatenate(([['id', 'prediction']], submission), axis=0)\n","\n","    return submission"]},{"cell_type":"code","execution_count":null,"id":"fcc3e105","metadata":{"id":"fcc3e105"},"outputs":[],"source":["submission = submission_creating(model, \n","                                 path_testing, \n","                                 training_resize,\n","                                 testing_resize,\n","                                 cuda_available)\n","\n","np.savetxt(\"submit.csv\", submission, delimiter=\",\", fmt = '%s')\n","if use_google_colab:\n","    files.download('submit.csv')"]},{"cell_type":"code","execution_count":null,"id":"24c4b4cb","metadata":{"id":"24c4b4cb"},"outputs":[],"source":["index = 23\n","model.eval()\n","\n","# Load a testing image\n","input = np.array(Image.open(f'{path_testing}/test_{index}/test_{index}.png')).astype('float32') / 255\n","\n","# Resize the testing image\n","input = resize(input, (testing_resize, testing_resize))\n","\n","# Divide the resized testing image into four patches, one at each corner.\n","input_patches = testing_patch_extracting(input, training_resize, testing_resize)\n","input_patches = torch.from_numpy(input_patches).float()\n","\n","# Predict the mask of the four patches\n","if cuda_available:\n","    output_patches = model(input_patches.cuda()).detach().cpu().numpy()\n","else:\n","    output_patches = model(input_patches).detach().numpy()\n","\n","# Merge the four masks into one resized mask\n","output = testing_patch_assembling(output_patches, training_resize, testing_resize)[0, :, :]\n","\n","# Restore the resized mask to the original resolution\n","output = resize(output, (608, 608))\n","\n","f, axs = plt.subplots(1, 2)\n","axs[0].imshow(input)\n","axs[1].imshow(output)"]},{"cell_type":"code","execution_count":null,"id":"Buz2mE2DA5pL","metadata":{"id":"Buz2mE2DA5pL"},"outputs":[],"source":["im = Image.fromarray(output * 255)\n","im = im.convert(\"L\")\n","im.save(\"mask.png\")\n","if use_google_colab:\n","    files.download('mask.png')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"LinkNet34.ipynb","provenance":[{"file_id":"1i_hTd12pCfVGEf3i-UeYeJW1trUuk_CT","timestamp":1640256050805}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":5}
